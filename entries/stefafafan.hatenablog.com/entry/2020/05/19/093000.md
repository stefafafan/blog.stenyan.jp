---
Title: OBSとWeb Audio APIを使って音に合わせてオーディオスペクトラムをWebカメラ映像に重ねて配信する
Category:
- 技術
Date: 2020-05-19T09:30:00+09:00
URL: https://blog.stenyan.jp/entry/2020/05/19/093000
EditURL: https://blog.hatena.ne.jp/stefafafan/stefafafan.hatenablog.com/atom/entry/26006613570070464
---

大したネタでもないですが、この前ふとこれやりたいなと思ってできたのでついでにブログに書いてみます。

** DJ配信

みなさんやってますよね。え、やってない？ではいますぐDJ機材買いましょう。
それはそうと、昨今STAY HOMEの影響でDJの人もどうやらクラブでイベント全然できなくなって、仕方ないからDJ配信するか～とTwitchなどで盛り上がってるようです。

僕はただのインドア系のWebアプリケーションエンジニアなので、DJなんてパリピなものよく知らないんですが、家にたまたまDJ機材があったので、配信とかしてみたいな～と思いはじめつつ、OBSというソフトをインストールしたりWebカメラをセットアップしたりしてみてるのですが、DJ中の自分の様子をカメラで写しても盛り上がりにかけることに気づきました。

<figure class="figure-image figure-image-fotolife" title="DJ機材の前でポーズをとっている男性">[f:id:stefafafan:20200518233650j:plain]<figcaption>DJ機材の前でポーズをとっている男性</figcaption></figure>

こんな映像がインターネットに流れてきても絶対にみようという気になれない。かといって、DJはDJしている間手が離せないはずで、DJしながら何かしら映像をリアルタイムで工夫する（VJをする）というのは難しそう。

と考えていった結果、「映像が勝手にいい感じになればよいのでは？」と気づきました。

** 前提

このへんのものが必要です。

- 音が鳴るものを持っている
- パソコンの音声入力にその音が鳴るものを流すことができる
- パソコンにGoogle Chromeなどのブラウザがインストールされている
- パソコンにOBSというソフトがインストールされている

** Web Audio APIを使ってマイクの音に合わせてオーディオスペクトラム的なやつを出す

これ絶対できるはず（大昔にどこかで見た記憶がある）ということでググった結果色々でてきた。

[https://1000ch.net/posts/2017/visualize-audio-input.html:embed:cite]

[https://qiita.com/soarflat/items/004cac51b818b9483764:embed:cite]

[http://man.hubwiz.com/docset/JavaScript.docset/Contents/Resources/Documents/developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API.html:embed:cite]

このへんのコード真似すれば完成します。色々試した結果、この3つ目のサイトの「Creating a frequency bar graph」が良さそうということで真似して作りました。
ローカルの適当なところに、htmlファイルとJSファイルとCSSファイル作って、開いたら全画面で波形が出るように作ります。

JSファイルについては、まずCanvas周りを初期化していきます。

>|javascript|
const canvas = document.getElementById('wave');
canvas.width = document.body.clientWidth;
canvas.height = document.body.clientHeight;
const drawContext = canvas.getContext('2d');
||<

オーディオストリーム取得するにはこういうふうにやればいけるそうです。

>|javascript|
navigator.mediaDevices.getUserMedia({
    audio: true,
    video: false
}).then(stream => {
    ...
});
||<

この中がむずくて雰囲気で書いてます。AudioContextからAnalyserNodeを作ると、ここから周波数などの値が取れる模様。dataArrayに入ったデータをもとに今回描画していきます。ここのポイントとしてはanalyserNode.fftSize = 128 の部分で、ここを2048みたいな数字にするとより細かいスペクトラムにできるようでした。ぼくは細かいのは微妙に感じたので今回128にしました。

>|javascript|
const audioContext = new AudioContext();
const sourceNode = audioContext.createMediaStreamSource(stream);
const analyserNode = audioContext.createAnalyser();
analyserNode.fftSize = 128;
sourceNode.connect(analyserNode);

var bufferLength = analyserNode.frequencyBinCount;
var dataArray = new Uint8Array(bufferLength);
||<

次にdraw関数を作ってその中でさっきのdataArrayの値をもとに描画していきます。アニメーションするときはrequestAnimationFrameが肝です。ここらへんも基本さっきのサイトを参考にしているんですが、setBackgroundColorByDataArrayっていうのが僕が新しく作った関数なのと、rgba ではなく hsl を使っているところがポイントです。hsl に i をもとにした値を指定してあげることによって、気軽に虹色っぽい感じにできるので、rgba より便利です。さっきのサイトのコードのままだと背景が単色で寂しいので、この上で、setBackgroundColorByDataArray という関数を作って音楽の感じによって背景色も変わるというのをやってみました。

>|javascript|
function draw () {
    drawVisual = requestAnimationFrame(draw);

    analyserNode.getByteFrequencyData(dataArray);
    var barWidth = (canvas.width / bufferLength) * 2.5;
    var barHeight;
    var x = 0;

    setBackgroundColorByDataArray(dataArray);

    for(var i = 0; i < bufferLength; i++) {
        barHeight = dataArray[i] * 2;
        drawContext.fillStyle = "hsl(" + i*3 + ",80%,50%)";
        drawContext.fillRect(x,canvas.height-barHeight,barWidth,barHeight);

        x += barWidth + 1;
    }
}
||<

setBackgroundColorByDataArrayをみてみるとものすごい雑なコードになってますが何がしたかったのかというと、「低音域が多い音がなってるときは背景を赤っぽく、高音域が多い音がなってるときは背景を青っぽく」したかった。
dataArrayの配列の中身をみていくと、最初に低音域の数値が入ってて、後ろにいくほど高音になるのがわかった。ということで、音域ごとの数値をすごいざっくり合計して、それをrgbaのそれぞれの値に適当に調整した値でマッピングしたら、勝手にいい感じになりました。その結果が以下のよくわからないマジックナンバーだらけのコードですね。

>|javascript|
function setBackgroundColorByDataArray (dataArray) {
    var highFrequencies = dataArray[60] +dataArray[61] +dataArray[62] + dataArray[63];
    var midFrequencies = dataArray[30] +dataArray[35] +dataArray[40] + dataArray[45];
    var lowFrequencies = dataArray[0] + dataArray[5] + dataArray[10] + dataArray[15];

    drawContext.fillStyle = 'rgba(' + lowFrequencies / 10 + ', '+ midFrequencies / 9 + ', '+  highFrequencies / 8 + ', 0.2)';
    drawContext.fillRect(0, 0, canvas.width, canvas.height);
}
||<

結果はこんな感じです。ブラウザで開いたときに最初にマイクの許可が必要ですと聞かれるので許可します。あと、入力として多分デフォルトのマイクとかが最初使われるけど、Google Chromeの設定とかで入力ソース切り替えできるので、僕はDJ機材からの入力を指定してあげました。

<figure class="figure-image figure-image-fotolife" title="きれいっぽい">[f:id:stefafafan:20200519000153p:plain]<figcaption>きれいっぽい</figcaption></figure>

** OBSの設定

OBSって何かというと動画や配信したいときにとても便利なソフトです。

[https://obsproject.com/ja:embed:cite]

OBS最初開くとこんな感じですが、ここでWebカメラの映像とかブラウザの画面をキャプチャしたものを貼り付けるとかテキストを乗せるとかできます。

<figure class="figure-image figure-image-fotolife" title="OBSの画面">[f:id:stefafafan:20200519000614p:plain]<figcaption>OBSの画面</figcaption></figure>

<figure class="figure-image figure-image-fotolife" title="Webカメラを追加した">[f:id:stefafafan:20200519000749p:plain]<figcaption>Webカメラを追加した</figcaption></figure>

このように画面に写ってるものをそのままキャプチャして追加することができます。このままだとぼくのChromeのブックマークバーとかが丸見えになるので適宜フィルタからクロッピングや透過レベル変えるなどしていきましょう

<figure class="figure-image figure-image-fotolife" title="ブラウザをキャプチャした">[f:id:stefafafan:20200519001123p:plain]<figcaption>ブラウザをキャプチャした</figcaption></figure>

<figure class="figure-image figure-image-fotolife" title="クロップやクロマキーや透明度などをいじってみてる">[f:id:stefafafan:20200519001312p:plain]<figcaption>クロップやクロマキーや透明度などをいじってみてる</figcaption></figure>

** ということで

完成したのがこれ。

<figure class="figure-image figure-image-fotolife" title="楽しそう">[f:id:stefafafan:20200519002920g:plain]<figcaption>楽しそう</figcaption></figure>

はい、よかったですね。

とにかくWeb Audio APIは便利そうです。こちらからは以上です。
